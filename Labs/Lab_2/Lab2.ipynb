{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 70,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "xRJq78yxmUL5",
        "outputId": "39868b11-c6f1-43f7-e089-84629cefcf1f"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ],
      "source": [
        "# Mount Google Drive\n",
        "from google.colab import drive\n",
        "drive.mount('/content/drive', force_remount=True)"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install pefile\n",
        "!pip install PyGitHub\n",
        "!pip install nltk\n",
        "!pip install scipy"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "collapsed": true,
        "id": "X0PYe2U_oXKH",
        "outputId": "fd72307c-72df-412c-a8fe-36a609e3fdc6"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting pefile\n",
            "  Downloading pefile-2024.8.26-py3-none-any.whl.metadata (1.4 kB)\n",
            "Downloading pefile-2024.8.26-py3-none-any.whl (74 kB)\n",
            "\u001b[?25l   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m0.0/74.8 kB\u001b[0m \u001b[31m?\u001b[0m eta \u001b[36m-:--:--\u001b[0m\r\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m74.8/74.8 kB\u001b[0m \u001b[31m4.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: pefile\n",
            "Successfully installed pefile-2024.8.26\n",
            "Collecting PyGitHub\n",
            "  Downloading PyGithub-2.6.1-py3-none-any.whl.metadata (3.9 kB)\n",
            "Collecting pynacl>=1.4.0 (from PyGitHub)\n",
            "  Downloading PyNaCl-1.5.0-cp36-abi3-manylinux_2_17_x86_64.manylinux2014_x86_64.manylinux_2_24_x86_64.whl.metadata (8.6 kB)\n",
            "Requirement already satisfied: requests>=2.14.0 in /usr/local/lib/python3.11/dist-packages (from PyGitHub) (2.32.3)\n",
            "Requirement already satisfied: pyjwt>=2.4.0 in /usr/local/lib/python3.11/dist-packages (from pyjwt[crypto]>=2.4.0->PyGitHub) (2.10.1)\n",
            "Requirement already satisfied: typing-extensions>=4.0.0 in /usr/local/lib/python3.11/dist-packages (from PyGitHub) (4.12.2)\n",
            "Requirement already satisfied: urllib3>=1.26.0 in /usr/local/lib/python3.11/dist-packages (from PyGitHub) (2.3.0)\n",
            "Requirement already satisfied: Deprecated in /usr/local/lib/python3.11/dist-packages (from PyGitHub) (1.2.18)\n",
            "Requirement already satisfied: cryptography>=3.4.0 in /usr/local/lib/python3.11/dist-packages (from pyjwt[crypto]>=2.4.0->PyGitHub) (43.0.3)\n",
            "Requirement already satisfied: cffi>=1.4.1 in /usr/local/lib/python3.11/dist-packages (from pynacl>=1.4.0->PyGitHub) (1.17.1)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.11/dist-packages (from requests>=2.14.0->PyGitHub) (3.4.1)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.11/dist-packages (from requests>=2.14.0->PyGitHub) (3.10)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.11/dist-packages (from requests>=2.14.0->PyGitHub) (2025.1.31)\n",
            "Requirement already satisfied: wrapt<2,>=1.10 in /usr/local/lib/python3.11/dist-packages (from Deprecated->PyGitHub) (1.17.2)\n",
            "Requirement already satisfied: pycparser in /usr/local/lib/python3.11/dist-packages (from cffi>=1.4.1->pynacl>=1.4.0->PyGitHub) (2.22)\n",
            "Downloading PyGithub-2.6.1-py3-none-any.whl (410 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m410.5/410.5 kB\u001b[0m \u001b[31m18.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading PyNaCl-1.5.0-cp36-abi3-manylinux_2_17_x86_64.manylinux2014_x86_64.manylinux_2_24_x86_64.whl (856 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m856.7/856.7 kB\u001b[0m \u001b[31m49.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: pynacl, PyGitHub\n",
            "Successfully installed PyGitHub-2.6.1 pynacl-1.5.0\n",
            "Requirement already satisfied: nltk in /usr/local/lib/python3.11/dist-packages (3.9.1)\n",
            "Requirement already satisfied: click in /usr/local/lib/python3.11/dist-packages (from nltk) (8.1.8)\n",
            "Requirement already satisfied: joblib in /usr/local/lib/python3.11/dist-packages (from nltk) (1.4.2)\n",
            "Requirement already satisfied: regex>=2021.8.3 in /usr/local/lib/python3.11/dist-packages (from nltk) (2024.11.6)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.11/dist-packages (from nltk) (4.67.1)\n",
            "Requirement already satisfied: scipy in /usr/local/lib/python3.11/dist-packages (1.14.1)\n",
            "Requirement already satisfied: numpy<2.3,>=1.23.5 in /usr/local/lib/python3.11/dist-packages (from scipy) (2.0.2)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**TASK 1**"
      ],
      "metadata": {
        "id": "8rncqKIzJFiR"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import sys\n",
        "import hashlib\n",
        "filename = \"/content/drive/MyDrive/ColabNotebooks/Lab_2/python-3.10.0-amd64.exe\"\n",
        "BUF_SIZE = 65536\n",
        "md5 = hashlib.md5()\n",
        "sha256 = hashlib.sha256()\n",
        "with open(filename, \"rb\") as f:\n",
        " while True:\n",
        "  data = f.read(BUF_SIZE)\n",
        "  if not data:\n",
        "    break\n",
        "  md5.update(data)\n",
        "  sha256.update(data)\n",
        "print(\"MD5: {0}\".format(md5.hexdigest()))\n",
        "print(\"SHA256: {0}\".format(sha256.hexdigest()))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "9Q4KDM4ypbDt",
        "outputId": "9f03a615-4022-4244-fcca-cdd734bf0ccb"
      },
      "execution_count": 50,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "MD5: c3917c08a7fe85db7203da6dcaa99a70\n",
            "SHA256: cb580eb7dc55f9198e650f016645023e8b2224cf7d033857d12880b46c5c94ef\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**TASK 8**"
      ],
      "metadata": {
        "id": "chKepM6Byc4t"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Bước 1 – Tạo list mẫu và gán nhãn**\n"
      ],
      "metadata": {
        "id": "6ZZqVZJIOp9m"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "from os import listdir\n",
        "directories_and_labels = [(\"/content/drive/MyDrive/ColabNotebooks/Lab_2/Benign PE Samples 2\", 0),\n",
        "(\"/content/drive/MyDrive/ColabNotebooks/Lab_2/Malicious PE Samples 2\", 1)]\n",
        "list_of_samples = []\n",
        "labels = []\n",
        "N_spec = 2 # For N-grams\n",
        "for dataset_path, label in directories_and_labels:\n",
        "  samples = [f for f in listdir(dataset_path)]\n",
        "  for sample in samples:\n",
        "    file_path = os.path.join(dataset_path, sample)\n",
        "    list_of_samples.append(file_path)\n",
        "    labels.append(label)\n",
        "print(list_of_samples[:5]) # Get the first 5 samples\n",
        "print(list_of_samples[len(list_of_samples) -5: len(list_of_samples)]) # Get the last 5 samples\n",
        "print(labels[:5]) # Get the first 5 samples\n",
        "print(labels[len(list_of_samples) -5: len(list_of_samples)]) # Get the last 5 samples\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "b6i_oru70OlV",
        "outputId": "ab78f09d-cc5d-4243-e2a8-e2eca3624897"
      },
      "execution_count": 71,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "['/content/drive/MyDrive/ColabNotebooks/Lab_2/Benign PE Samples 2/chkntfs.exe', '/content/drive/MyDrive/ColabNotebooks/Lab_2/Benign PE Samples 2/ComSvcConfig.exe', '/content/drive/MyDrive/ColabNotebooks/Lab_2/Benign PE Samples 2/ComputerDefaults.exe', '/content/drive/MyDrive/ColabNotebooks/Lab_2/Benign PE Samples 2/CompMgmtLauncher.exe', '/content/drive/MyDrive/ColabNotebooks/Lab_2/Benign PE Samples 2/CompatTelRunner.exe']\n",
            "['/content/drive/MyDrive/ColabNotebooks/Lab_2/Malicious PE Samples 2/5a765351046fea1490d20f25.exe', '/content/drive/MyDrive/ColabNotebooks/Lab_2/Malicious PE Samples 2/abba_-_happy_new_year_zaycev_net.exe', '/content/drive/MyDrive/ColabNotebooks/Lab_2/Malicious PE Samples 2/7ZipSetup.exe', '/content/drive/MyDrive/ColabNotebooks/Lab_2/Malicious PE Samples 2/gchrome.exe', '/content/drive/MyDrive/ColabNotebooks/Lab_2/Malicious PE Samples 2/aapt.exe']\n",
            "[0, 0, 0, 0, 0]\n",
            "[1, 1, 1, 1, 1]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Bước 2 – Chia tập Train/Test theo tỷ lệ 70% Train – 30% Test**"
      ],
      "metadata": {
        "id": "Jb1httMVJXug"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.model_selection import train_test_split\n",
        "samples_train, samples_test, target_train, target_test = train_test_split(\n",
        "  list_of_samples,\n",
        "  labels,\n",
        "  test_size=0.3,\n",
        "  stratify = labels,\n",
        "  random_state = 11\n",
        ")"
      ],
      "metadata": {
        "id": "pTsqmlwYJZL5"
      },
      "execution_count": 72,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Bước 3 – Xây dựng hàm trích xuất thuộc tính**"
      ],
      "metadata": {
        "id": "JPooWRG0JfXk"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import collections\n",
        "from nltk import ngrams\n",
        "import numpy as np\n",
        "import pefile\n",
        "def read_file(file_path):\n",
        "  with open(file_path, \"rb\") as bin_file:\n",
        "    data = bin_file.read()\n",
        "    return data\n",
        "\n",
        "def byte_seq_to_Ngrams(byte_seq, N_par):\n",
        "  Ngrams_par = ngrams(byte_seq, N_par)\n",
        "  return list(Ngrams_par)\n",
        "\n",
        "def bin_file_to_Ngrams_count(file_path, N_par):\n",
        "  file_seq = read_file(file_path)\n",
        "  file_Ngrams = byte_seq_to_Ngrams(file_seq, N_par)\n",
        "  return collections.Counter(file_Ngrams)\n",
        "\n",
        "def get_Ngrams_features_from_samples(sample, K1_most_freq_Ngrams_list):\n",
        "  K1 = len(K1_most_freq_Ngrams_list)\n",
        "  feature_vector = K1 * [0]\n",
        "  file_Ngrams = bin_file_to_Ngrams_count(sample, N_spec)\n",
        "  for i in range(K1):\n",
        "    feature_vector[i] = file_Ngrams[K1_most_freq_Ngrams_list[i]]\n",
        "  return feature_vector\n",
        "\n",
        "def preprocess_imports(list_of_DLLs):\n",
        "  \"\"\" Normalize the name of the imports of a PE file. \"\"\"\n",
        "  temp = [x.decode().split(\".\")[0].lower() for x in list_of_DLLs] # View the transforming of below example\n",
        "  return \" \".join(temp)\n",
        "\n",
        "def get_imports(pe):\n",
        "  \"\"\" Get a list of the imports of a PE file \"\"\"\n",
        "  list_of_imports = []\n",
        "  for entry in pe.DIRECTORY_ENTRY_IMPORT:\n",
        "    list_of_imports.append(entry.dll)\n",
        "  return preprocess_imports(list_of_imports)\n",
        "\n",
        "\n",
        "def get_section_names(pe):\n",
        "  \"\"\" Get a list of the section names of a PE file \"\"\"\n",
        "  list_of_sections = []\n",
        "  for sect in pe.sections:\n",
        "    normalized_name = sect.Name.decode().replace(\"\\x00\", \"\").lower()\n",
        "    list_of_sections.append(normalized_name)\n",
        "  return \"\".join(list_of_sections)"
      ],
      "metadata": {
        "id": "NuFyLMBfJgEX"
      },
      "execution_count": 73,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Bước 4 – Lấy 100 N-grams phổ biến nhất**"
      ],
      "metadata": {
        "id": "DiGEQ_Y3J2VZ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "Ngrams_count_all = collections.Counter([])\n",
        "for sample in samples_train:\n",
        "  Ngrams_count_all += bin_file_to_Ngrams_count(sample, N_spec)\n",
        "K1 = 100\n",
        "K1_most_common_Ngrams = Ngrams_count_all.most_common(K1)\n",
        "K1_most_common_Ngrams_list = [x[0] for x in K1_most_common_Ngrams]"
      ],
      "metadata": {
        "id": "usKIZQ1SJ4Ru"
      },
      "execution_count": 74,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Bước 5 – Trích xuất thuộc tính cho tập Train**\n"
      ],
      "metadata": {
        "id": "WFf1836fJ6Sl"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "imports_corpus_train = []\n",
        "num_sect_train = []\n",
        "sect_name_train = []\n",
        "Ngram_feat_list_train = []\n",
        "\n",
        "y_train = []"
      ],
      "metadata": {
        "id": "LXQgheNSJ65B"
      },
      "execution_count": 75,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "for i in range(len(samples_train)):\n",
        "  sample = samples_train[i]\n",
        "  try:\n",
        "    # Get all required parameters with predefined functions\n",
        "    Ngram_features = get_Ngrams_features_from_samples(sample, K1_most_common_Ngrams_list)\n",
        "    pe = pefile.PE(sample)\n",
        "    imports = get_imports(pe)\n",
        "    n_sections = len(pe.sections)\n",
        "    sec_names = get_section_names(pe)\n",
        "\n",
        "    # Put above value into lists\n",
        "    imports_corpus_train.append(imports)\n",
        "    num_sect_train.append(n_sections)\n",
        "    sect_name_train.append(sec_names)\n",
        "    Ngram_feat_list_train.append(Ngram_features)\n",
        "\n",
        "    # Target train\n",
        "    y_train.append(target_train[i])\n",
        "  except Exception as e:\n",
        "    print(sample + \":\")\n",
        "    print(e)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "9GBPsHn9J-Hg",
        "outputId": "39fc97f9-5929-45a6-c033-59e10744a3d6"
      },
      "execution_count": 76,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "/content/drive/MyDrive/ColabNotebooks/Lab_2/Benign PE Samples 2/cmak.exe:\n",
            "'DOS Header magic not found.'\n",
            "/content/drive/MyDrive/ColabNotebooks/Lab_2/Benign PE Samples 2/dcdiag.exe:\n",
            "'DOS Header magic not found.'\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Bước 6 - Sử dụng hàm băm tfidf để chuyển imports, section names từ văn bản thành dạng số**\n"
      ],
      "metadata": {
        "id": "9aZnKWtKJ__p"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.feature_extraction.text import HashingVectorizer, TfidfTransformer\n",
        "from sklearn.pipeline import Pipeline"
      ],
      "metadata": {
        "id": "TWHhfhXuKBTe"
      },
      "execution_count": 77,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "imports_featurizer = Pipeline(\n",
        "    [\n",
        "        (\"vect\", HashingVectorizer(input = \"content\", ngram_range=(1,2))),\n",
        "        (\"tfidf\", TfidfTransformer(use_idf = True,)),\n",
        "    ]\n",
        ")"
      ],
      "metadata": {
        "id": "LsKbCj2oKHUL"
      },
      "execution_count": 78,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "sect_name_featurizer = Pipeline(\n",
        "    [\n",
        "        (\"vect\", HashingVectorizer(input = \"content\", ngram_range= (1,2))),\n",
        "        (\"tfidf\", TfidfTransformer(use_idf = True))\n",
        "    ]\n",
        ")"
      ],
      "metadata": {
        "id": "xO27c4k-KJvF"
      },
      "execution_count": 79,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "imports_corpus_train_transformed = imports_featurizer.fit_transform(imports_corpus_train)\n",
        "sect_name_train_transformed = sect_name_featurizer.fit_transform(sect_name_train)\n"
      ],
      "metadata": {
        "id": "C14flQG2KLoW"
      },
      "execution_count": 80,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Bước 7 - Kết hợp các vector thuộc tính thành 1 mảng**\n"
      ],
      "metadata": {
        "id": "L2Cs0I0KKPEO"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from scipy.sparse import hstack, csr_matrix\n"
      ],
      "metadata": {
        "id": "hBaUNDpuKPjy"
      },
      "execution_count": 81,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "X_train = hstack(\n",
        "    [\n",
        "        Ngram_feat_list_train,\n",
        "        imports_corpus_train_transformed,\n",
        "        sect_name_train_transformed,\n",
        "        csr_matrix(num_sect_train).transpose(),\n",
        "    ]\n",
        ")\n"
      ],
      "metadata": {
        "id": "bzMqHxkMKRZT"
      },
      "execution_count": 82,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Bước 8 - Huấn luyện  Random Forest cho tập **\n",
        "\n"
      ],
      "metadata": {
        "id": "r6Yp3wK0KVLf"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.ensemble import RandomForestClassifier\n"
      ],
      "metadata": {
        "id": "lDhhJxVnKVtd"
      },
      "execution_count": 83,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "clf = RandomForestClassifier(n_estimators = 100)\n",
        "clf = clf.fit(X_train, y_train)"
      ],
      "metadata": {
        "id": "BLP7fcFKKYSF"
      },
      "execution_count": 84,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Bước 9 - Thu thập các thuộc tính của tập test, giống như tập huấn luyện**"
      ],
      "metadata": {
        "id": "wzDz2l8pKZ6L"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import_corpus_test = []\n",
        "num_sect_test = []\n",
        "sect_names_test = []\n",
        "Ngram_feat_list_test = []\n",
        "\n",
        "y_test = []"
      ],
      "metadata": {
        "id": "s8GHnnKyKbop"
      },
      "execution_count": 85,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "for i in range(len(samples_test)):\n",
        "  test = samples_test[i]\n",
        "  try:\n",
        "    # Get all required parameters with predefined functions\n",
        "    # The input when getting N-grams features is still \"sample\"\n",
        "    Ngram_features = get_Ngrams_features_from_samples(sample, K1_most_common_Ngrams_list)\n",
        "    pe = pefile.PE(test) # Get test PE file\n",
        "    imports = get_imports(pe)\n",
        "    n_sections = len(pe.sections)\n",
        "    sec_names = get_section_names(pe)\n",
        "\n",
        "    # Put above value into lists\n",
        "    import_corpus_test.append(imports)\n",
        "    num_sect_test.append(n_sections)\n",
        "    sect_names_test.append(sec_names)\n",
        "    Ngram_feat_list_test.append(Ngram_features)\n",
        "\n",
        "    # Target train\n",
        "    y_test.append(target_test[i])\n",
        "  except Exception as e:\n",
        "    print(sample + \":\")\n",
        "    print(e)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "QDpR9xkCKdXg",
        "outputId": "361e2964-e420-4309-c6b1-401ca24a98fb"
      },
      "execution_count": 86,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "/content/drive/MyDrive/ColabNotebooks/Lab_2/Benign PE Samples 2/CloudExperienceHostBroker.exe:\n",
            "'DOS Header magic not found.'\n",
            "/content/drive/MyDrive/ColabNotebooks/Lab_2/Benign PE Samples 2/CloudExperienceHostBroker.exe:\n",
            "'DOS Header magic not found.'\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "y_test\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "collapsed": true,
        "id": "_nZf8KgAKfsY",
        "outputId": "a19fc2f3-08fd-4108-a459-ae565f70b507"
      },
      "execution_count": 87,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 1,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 1,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0]"
            ]
          },
          "metadata": {},
          "execution_count": 87
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Bước 10 - Chuyển đổi vector từ thuộc tính test, và kiểm tra kết quả của trình phân loại**\n"
      ],
      "metadata": {
        "id": "zCGjysx3Lwdx"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import_corpus_test_transformed = imports_featurizer.transform(import_corpus_test)\n",
        "sect_names_test_transformed = imports_featurizer.transform(sect_names_test)\n",
        "X_test = hstack(\n",
        "    [\n",
        "        Ngram_feat_list_test,\n",
        "        import_corpus_test_transformed,\n",
        "        sect_names_test_transformed,\n",
        "        csr_matrix(num_sect_test).transpose()\n",
        "    ]\n",
        ")"
      ],
      "metadata": {
        "id": "9xn3IfcDLzLA"
      },
      "execution_count": 90,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(\"The score of our classifier is as follow: \")\n",
        "print(clf.score(X_test, y_test))\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "BiqNick6MGhf",
        "outputId": "7ca9a78d-d43f-4a35-8320-9bc60b49b771"
      },
      "execution_count": 91,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "The score of our classifier is as follow: \n",
            "0.9310344827586207\n"
          ]
        }
      ]
    }
  ]
}